{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of MejorandoRendimiento.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEGoCRkOIRDYK00Y8aAprq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sap0408/2048/blob/master/Copy_of_Copy_of_MejorandoRendimiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59_vr2QC-k2C",
        "colab_type": "code",
        "outputId": "91b0ecd2-2225-478e-d208-94370d5684f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.4)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.29.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQlMG9PwUYU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgPaA3Qs-oGI",
        "colab_type": "code",
        "outputId": "9271e7c4-e105-4954-ae6f-7516e1314c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print (tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPit2ODqK_7o",
        "colab_type": "code",
        "outputId": "8fe8fde6-a857-4b2d-8343-05cbd3a6772b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fSgLO-wLID5",
        "colab_type": "code",
        "outputId": "d4a08fa8-f802-46fe-a1a7-1252b89f82d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#One hot con palabras\n",
        "import numpy as np\n",
        "\n",
        "# En este ejemplo de juguete, una \"muestra\" es solo una oración, pero\n",
        "# podría ser un documento completo.\n",
        "samples = ['El gato rasguño mi alfonbra.', 'El perro se comio mi tarea.']\n",
        "\n",
        "# Creamos índice para todos los tokens en los datos.\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "    # Simplemente tokenizamos las muestras mediante el método `split`. En la vida real, \n",
        "    # también eliminaríamos la puntuación y los caracteres especiales de las muestras.\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            # Asignar un índice único a cada palabra única\n",
        "            token_index[word] = len(token_index) + 1\n",
        "            # Tenga en cuenta que no atribuimos el índice 0 a nada.\n",
        "print(token_index)\n",
        "\n",
        "# A continuación, vectorizamos nuestras muestras.\n",
        "# Solo consideraremos las primeras palabras `max_length` en cada muestra.\n",
        "max_length = 5\n",
        "\n",
        "# Aquí es donde almacenamos nuestros resultados:\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1.\n",
        "print(results) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'El': 1, 'gato': 2, 'rasguño': 3, 'mi': 4, 'alfonbra.': 5, 'perro': 6, 'se': 7, 'comio': 8, 'tarea.': 9}\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m4jskAwLik7",
        "colab_type": "code",
        "outputId": "01e2ce64-7667-4233-92fd-0223d95b05ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "#One hot con caracteres\n",
        "import string\n",
        "\n",
        "samples = ['El gato rasguño mi alfonbra.', 'El perro se comio mi tarea.']\n",
        "\n",
        "characters = string.printable  # Todos los caracteres ASCII imprimibles.\n",
        "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
        "\n",
        "print(token_index)\n",
        "\n",
        "max_length = 3 #50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, character in enumerate(sample[:max_length]):\n",
        "        index = token_index.get(character)\n",
        "        results[i, j, index] = 1.\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': 41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': 51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': 61, 'Z': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': 71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': 81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': 91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100}\n",
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 1. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QGZASKqLyj7",
        "colab_type": "code",
        "outputId": "b43d1ab6-5303-4c77-af9d-aa8207f9d79b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#One hot con palabras, PERO usando Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['El gato rasguño mi alfonbra.', 'El perro se comio mi tarea.']\n",
        "\n",
        "# Creamos un tokenizador, configurado para tomar solo\n",
        "# en cuenta las 20 palabras más comunes\n",
        "tokenizer = Tokenizer(num_words=20)\n",
        "# This builds the word index\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "\n",
        "# Esto convierte las cadenas en listas de índices enteros.\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "print(samples)\n",
        "print(sequences)\n",
        "\n",
        "# También puede obtener directamente las representaciones binarias.\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "\n",
        "# Así es como puede recuperar el índice de palabras que se calculó\n",
        "word_index = tokenizer.word_index\n",
        "print('Se encontrarom %s tokens unicos.' % len(word_index))\n",
        "print(one_hot_results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['El gato rasguño mi alfonbra.', 'El perro se comio mi tarea.']\n",
            "[[1, 3, 4, 2, 5], [1, 6, 7, 8, 2, 9]]\n",
            "Se encontrarom 9 tokens unicos.\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mfv0r3MMOnC",
        "colab_type": "code",
        "outputId": "c3f6142b-487f-4d15-a8e4-c8f4192810fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#Codificación hash\n",
        "#One hot hash con palabras\n",
        "samples = ['El gato rasguño mi alfonbra.', 'El perro se comio mi tarea.']\n",
        "\n",
        "# Almacenaremos nuestras palabras como vectores de tamaño 10.\n",
        "# Tenga en cuenta que si tiene cerca de 10 palabras (o más)\n",
        "# comenzará a ver muchas colisiones de hash, que\n",
        "# disminuirá la precisión de este método de codificación.\n",
        "dimensionality = 5\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        # Hashear la palabra en un índice entero \"aleatorio\" que está entre 0 y 10\n",
        "        index = abs(hash(word)) % dimensionality\n",
        "        results[i, j, index] = 1.\n",
        "results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4fI2S6NMiyz",
        "colab_type": "code",
        "outputId": "bbe76f5d-4d8a-4127-dced-03331056d280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#Usar vectores de palabras (embeddings)\n",
        "#Capa embedding de Keras\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10, 3, input_length=5))\n",
        "# 5 entradas de numeros menosres a 10 se trasnforman en 5 vectores de 3 conponenetes\n",
        "# El modelo tomará como entrada una matriz entera de tamaño (lote, 5)\n",
        "# y el entero más grande en cada entrada no debe ser mayor que 999 (tamaño de vocabulario).\n",
        "# Ahora model.output_shape es (None, 5, 10), donde `None` es el lote.\n",
        "model.compile('rmsprop', 'mse')\n",
        "output_array = model.predict(np.array([[1, 8, 7, 5, 4]]))\n",
        "print(\"Salida: \"+ str(output_array))\n",
        "print(\"\\nTamano: \"+ str(output_array.shape)+\"\\n\")\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salida: [[[-4.5827854e-02  1.0089390e-03 -4.5176517e-02]\n",
            "  [ 3.9318312e-02 -2.3258125e-02 -1.1512265e-03]\n",
            "  [-2.0147717e-02 -1.5355658e-02 -4.2808153e-02]\n",
            "  [ 5.3420663e-06  8.4328279e-03  4.1656028e-02]\n",
            "  [ 4.0496740e-02 -1.7721556e-02  5.8554523e-03]]]\n",
            "\n",
            "Tamano: (1, 5, 3)\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 5, 3)              30        \n",
            "=================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Uu1hFnM6yK",
        "colab_type": "code",
        "outputId": "acc498ff-b3ef-4f58-9267-1f63c6f9c9d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#EJEMPLO IMDB\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "# Número de palabras a considerar\n",
        "max_features = 10000\n",
        "\n",
        "# Cortar textos después de este número de palabras\n",
        "maxlen = 40\n",
        "\n",
        "# Cargue los datos como listas de enteros.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(\"Train x: \"+str(x_train.shape))\n",
        "print(\"Test x: \"+str(x_test.shape))\n",
        "print(\"Train y: \"+str(y_train.shape))\n",
        "print(\"Test y: \"+str(y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Train x: (25000,)\n",
            "Test x: (25000,)\n",
            "Train y: (25000,)\n",
            "Test y: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP-t1DrNNXG4",
        "colab_type": "code",
        "outputId": "feca02db-33bc-4650-e8cf-931924f4c70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "resena=5\n",
        "print(\"\\nOpinion 11: \"+str(x_train[resena]))\n",
        "print(\"\\nSentimiento 11: \"+str(y_train[resena]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Opinion 11: [1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 2, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 2, 10, 10]\n",
            "\n",
            "Sentimiento 11: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVExUJ50Nka4",
        "colab_type": "code",
        "outputId": "a6c44b2e-b7e5-4b4b-b2a6-218f3b826f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Lo revertimos, mapeando índices enteros a palabras.\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# Decodificamos la resena; tenga en cuenta que nuestros índices fueron compensados por 3\n",
        "# porque 0, 1 y 2 son índices reservados para \"relleno\", \"inicio de secuencia\" y \"desconocido\".\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[resena]])\n",
        "decoded_review"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"? begins better than it ends funny that the russian submarine crew ? all other actors it's like those scenes where documentary shots br br spoiler part the message ? was contrary to the whole story it just does not ? br br\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnz8NfVANuzq",
        "colab_type": "code",
        "outputId": "78fdcc91-0227-4442-9f0f-313b347ae21a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# Esto convierte nuestras listas de enteros en un tensor de forma 2D `(samples, maxlen)`\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print(\"Train x: \"+str(x_train.shape))\n",
        "print(\"Test x: \"+str(x_test.shape))\n",
        "print(\"Train y: \"+str(y_train.shape))\n",
        "print(\"Test y: \"+str(y_test.shape))\n",
        "print(\"Opinion 11: \"+str(x_train[resena][:]))\n",
        "print(\"Sentimiento 11: \"+str(y_train[1]))\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[resena][:]])\n",
        "decoded_review"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train x: (25000, 40)\n",
            "Test x: (25000, 40)\n",
            "Train y: (25000,)\n",
            "Test y: (25000,)\n",
            "Opinion 11: [  74   12  630  163   15    4 1766 7982 1051    2   32   85  156   45\n",
            "   40  148  139  121  664  665   10   10 1361  173    4  749    2   16\n",
            " 3804    8    4  226   65   12   43  127   24    2   10   10]\n",
            "Sentimiento 11: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"than it ends funny that the russian submarine crew ? all other actors it's like those scenes where documentary shots br br spoiler part the message ? was contrary to the whole story it just does not ? br br\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLZqJkHCN6jN",
        "colab_type": "code",
        "outputId": "5b9bc005-9350-4ab3-c323-051e6f45c3e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Especificamos la longitud máxima de entrada a nuestra capa de incrustación  para que\n",
        "# luego podamos aplanar las entradas incrustadas\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "\n",
        "# Después de la capa de incrustación, nuestras activaciones tienen forma `(samples, maxlen, 8)`.\n",
        "\n",
        "# Aplanamos el tensor 3D de incrustaciones en un tensor 2D `(samples, maxlen * 8)`\n",
        "model.add(Flatten())\n",
        "\n",
        "# Agregamos el clasificador en la parte superior\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 40, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 320)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 321       \n",
            "=================================================================\n",
            "Total params: 80,321\n",
            "Trainable params: 80,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg67IaXYODZa",
        "colab_type": "code",
        "outputId": "db20b474-c713-43a9-e14b-122edcbadfb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 2s 79us/step - loss: 0.6506 - acc: 0.6532 - val_loss: 0.5618 - val_acc: 0.7472\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 1s 67us/step - loss: 0.4731 - acc: 0.7945 - val_loss: 0.4574 - val_acc: 0.7818\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 1s 65us/step - loss: 0.3905 - acc: 0.8293 - val_loss: 0.4317 - val_acc: 0.7920\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 1s 65us/step - loss: 0.3518 - acc: 0.8470 - val_loss: 0.4272 - val_acc: 0.8008\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 1s 65us/step - loss: 0.3247 - acc: 0.8613 - val_loss: 0.4275 - val_acc: 0.8016\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 1s 65us/step - loss: 0.3027 - acc: 0.8717 - val_loss: 0.4305 - val_acc: 0.8004\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 1s 64us/step - loss: 0.2825 - acc: 0.8849 - val_loss: 0.4385 - val_acc: 0.7998\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 1s 63us/step - loss: 0.2639 - acc: 0.8932 - val_loss: 0.4431 - val_acc: 0.7998\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 1s 64us/step - loss: 0.2453 - acc: 0.9027 - val_loss: 0.4530 - val_acc: 0.7968\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 1s 65us/step - loss: 0.2282 - acc: 0.9110 - val_loss: 0.4644 - val_acc: 0.7952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d30Jy9h6waWg",
        "colab_type": "code",
        "outputId": "083cb885-7b65-4a25-83c2-baea952a54d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"aclImdb.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBP6wQnkxyJx",
        "colab_type": "code",
        "outputId": "5510e2ac-039d-47af-8495-aebaae790020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"glove.6B.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c05b6060e6f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.6B.zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9ckxYbncO11",
        "colab_type": "code",
        "outputId": "685c635c-aa18-405d-d91d-aeb52c0b27e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = os.getcwd()\n",
        "train_dir = os.path.join(imdb_dir, 'aclImdb', 'train')\n",
        "print(train_dir)\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/aclImdb/train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTL1yGJ90irU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "76c77613-77b1-4cf6-f887-e64c87e0a7b0"
      },
      "source": [
        "print(\"\\nNumero de opiniones de entraenamiento: \"+str(len(texts)))    \n",
        "print(\"\\nNumero de labels: \"+str(len(texts)))  \n",
        "print(\"\\nEjemplo:\\n \"+str(texts[5]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Numero de opiniones de entraenamiento: 25000\n",
            "\n",
            "Numero de labels: 25000\n",
            "\n",
            "Ejemplo:\n",
            " Watching this Movie? l thought to myself, what a lot of garbage. These girls must have rocks for brains for even agreeing to be part of it. Waste of time watching it, faint heavens l only hired it. The acting was below standard and story was unbearable. Anyone contemplating watching this film, please save your money. The film has no credit at all. l am a real film buff and this is worse than \"Attack of the Green Tomatoes\".<br /><br />l only hope that this piece of trash didn't cost too much to make. Money would have been better spent on the homeless people of the world. l only hope there isn't a sequel in the pipeline.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACroc4ru0ycW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1bfc8df7-a26e-4e0c-e9b6-6375e77775eb"
      },
      "source": [
        "#tokenizar datos\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # Cortaremos las reseñas después de 100 palabras.\n",
        "training_samples = 200  # Entrenaremos con 200 muestras.\n",
        "validation_samples = 10000  # Validaremos en 10000 muestras.\n",
        "max_words = 10000  # Solo consideraremos las 10.000 palabras principales en el conjunto de datos\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Se encontraron %s tokens unicos.' % len(word_index))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Se encontraron 88582 tokens unicos.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCdIS-Mf1G6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d09fc136-21ab-4c60-966a-8c6982761fc3"
      },
      "source": [
        "print(list(tokenizer.word_counts)[:50])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hayden', 'christianson', 'and', 'jessica', 'alba', 'two', 'of', 'my', 'least', 'favourite', 'actors', 'this', 'century', 'team', 'up', 'in', 'what', 'is', 'quite', 'possibly', 'the', 'flattest', 'attempt', 'at', 'remake', 'already', 'dire', 'butterfly', 'effect', 'awake', 'so', 'dull', 'utterly', 'uninteresting', 'that', \"you'd\", 'be', 'better', 'off', 'asleep', 'terrance', 'howard', 'still', 'recovering', 'from', 'diabolical', 'august', 'rush', 'puts', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKYPXUKC1M3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "05b12835-b2d7-42e1-a6a6-15d33588d180"
      },
      "source": [
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Forma del tensor de datos:', data.shape)\n",
        "print('Forma del tensor de etiquetas:', labels.shape)\n",
        "\n",
        "# Divide los datos en un conjunto de entrenamiento y un conjunto de validación\n",
        "# Pero primero, baraja los datos, ya que comenzamos con los datos como se \n",
        "# ordenan las muestras (todas negativas primero, luego todas positivas).\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Forma del tensor de datos: (25000, 100)\n",
            "Forma del tensor de etiquetas: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}